{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuZCW+8aIZZ7H1GEkPnwGA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RanjanTarun27/text-to-disease/blob/main/text_to_disease.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q transformers torch gradio matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "\n",
        "class ResumeReadyClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.dropouts = nn.ModuleList([nn.Dropout(0.3) for _ in range(5)])\n",
        "        self.classifier = nn.Linear(768, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "\n",
        "\n",
        "        attn_weights = F.softmax(self.attention(sequence_output), dim=1)\n",
        "        context_vector = torch.sum(attn_weights * sequence_output, dim=1)\n",
        "\n",
        "\n",
        "        logits = torch.mean(torch.stack([\n",
        "            self.classifier(d(context_vector)) for d in self.dropouts\n",
        "        ], dim=0), dim=0)\n",
        "\n",
        "        return logits, attn_weights\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "TOKENIZER = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "\n",
        "CLASSES = [\"Covid-19\", \"Influenza\", \"Common Cold\", \"Pneumonia\", \"Bronchitis\", \"Allergy\"]\n",
        "model = ResumeReadyClassifier(n_classes=len(CLASSES)).to(DEVICE).eval()\n",
        "\n",
        "\n",
        "def predict_symptoms(text):\n",
        "    inputs = TOKENIZER(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, weights = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "        probabilities = F.softmax(logits, dim=1).flatten()\n",
        "\n",
        "\n",
        "    confidences = {CLASSES[i]: float(probabilities[i]) for i in range(len(CLASSES))}\n",
        "\n",
        "\n",
        "    tokens = TOKENIZER.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "    attn_scores = weights[0].cpu().numpy().flatten()\n",
        "\n",
        "\n",
        "    word_importance = \"\"\n",
        "    for token, score in zip(tokens, attn_scores):\n",
        "        if token not in ['[CLS]', '[SEP]', '[PAD]']:\n",
        "            word_importance += f\"{token} ({score:.2f})  \"\n",
        "\n",
        "    return confidences, f\"Key Symptoms Detected: {word_importance}\"\n",
        "\n",
        "\n",
        "desc = \"\"\"\n",
        "## ðŸ©º Advanced Medical Symptom Classifier\n",
        "**Features:** BERT-Base-Cased, Self-Attention Mechanism, Multi-Sample Dropout.\n",
        "*Type symptoms below to see the model's diagnosis and its confidence levels.*\n",
        "\"\"\"\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=predict_symptoms,\n",
        "    inputs=gr.Textbox(lines=3, placeholder=\"Describe symptoms (e.g., High fever, dry cough, and fatigue)...\"),\n",
        "    outputs=[gr.Label(num_top_classes=3, label=\"Top Diagnoses\"), gr.Markdown(label=\"Explainability\")],\n",
        "    title=\"Clinical Decision Support LLM\",\n",
        "    description=desc,\n",
        "    theme=\"soft\",\n",
        "    examples=[\n",
        "        [\"Sudden loss of taste and smell with a mild fever.\"],\n",
        "        [\"Wheezing and shortness of breath after exercise.\"]\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "interface.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "DMk_LW8LIn-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dE4TjcMuIoAv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}